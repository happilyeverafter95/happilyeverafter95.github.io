---
layout: post
title: Putting the future of artificial intelligence to the test (Exploring GPT-3)
---

_The title and following introduction was written entirely by GPT-3._

> There are many ideas about the future of artificial intelligence. The capabilities of computer systems are rapidly increasing at an exponential rate, and with that comes the question of how far they will go. There are many advancements that are taking place in computer science, with natural language processing being one of the most promising. \n\nFor example, natural language processing uses computer algorithms to find patterns in language, similar to how humans process language. These patterns are how computers derive meaning, and this technique can be used to accomplish almost any task. One reason for the prevalence of natural language processing is because many of the tasks computers are able to perform require complicated equations or large datasets, both of which are easier to analyze than language. \n\nWhile the future of artificial intelligence is unclear, advancements in natural language processing combined with other technologies will inevitably lead to incredible discoveries.

When I first heard about GPT-3 last July, I was skeptical. I had spent the past few days exploring GPT-2 and found it cool, but impractical. I couldn't understand how a gimmicky black box model could ever be useful in practice.

GPT-3 has exceeded all of my expectations. It's completely different from its predecessor and it is apparent that the developers put way more work here in addition to data and compute. GPT-3 actually generates useful information, relevant text and also does a much better job filtering out offensive notions and general nonsense. I've seen it being used various ways in the industry -- from content generation to an out of the box zero learning set up for other NLP tasks.
---

GPT-3 is an autoregressive language model that uses deep learning to produce text similarly to humans. There are numerous examples circulating the internet around some of the cool things it can do. Here is one where it is used to translate a plain English description to Javascript.

<p align="center">
<img src="/images/gpt_javascript.png" alt="English to Javascript" width="600"/>
</p>

My only concern is around how GPT-3 handles individual privacy. Can it be primed to return phone number or address look up? There's an [article on the Google Blog](https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html) that explored these concerns using GPT-2. **TLDR; 600/1800 sequences were memorized from the public training data which covered wide range of topics from news headlines, log messages, JavaScript code and PII**.

When I tried to prompt GPT-3 to return some personal information about myself, it responded with quite a bit of sass.

> Look, I'm busy.\nTell me everything about Mandy Gu.

In my earlier example, I instructed GPT-3 to write a catchy title and introduction for this post (why didn't I just instruct it to write the entire post?). These were the instructions I provided for text generation to the `davinci-instruct-beta` model (this model is fine tuned to complete prompts as if they were instructions).

```
Write an article talking about the future of natural language processing and artificial intelligence, with a focus around advancements. Make the opening sentence catchy and include a title.
```

Honestly, not a bad title and opener. As someone who likes to share my thoughts but isn't a polished writer, this could potentially be useful for myself in the future.

To gain access to the GPT-3 API, you have to join the [waitlist](https://openai.com/blog/openai-api/). I applied to this list last September and never got a response. I applied this Monday and got the access the next day. Yes, I have been fairly preoccupied lately exploring the API. 

Here's what I did differently:

1. Used an organization email address instead of my personal email
2. The first time I was honest and said I wanted explore the capabilities of GPT-3 without any projects in mind; this time, I thought carefully about applications

I am excited to start building some apps, but the service is not cheap.

<p align="center">
<img src="/images/pricing.png" alt="pricing" width="800"/>
</p>

However, using their API provides a very seamless experience for language generation. When it comes to producing content, the API costs are also a lot less than paying a human.

The only work we need to do is to _prime_ the model. Priming is the practice of providing an initial prompt to the language model to improve subsequent model predictions.

GPT-3 generally does very well even with short instructions and a few examples of your intended use case. Examples are typically delimited based on input and output. For instance, GPT-3 can be used to predict food ingredients based on the following prompt:

```
Given the name of a food, list the ingredients used to make this meal.

Food: apple pie
Ingredients: apple, butter, flour, egg, cinnamon, crust, sugar

Food: guacamole
Ingredients: avocado, tomato, onion, lime, salt
```

Following this guideline, I wrote a quick [Python library](https://pypi.org/project/gpt3-simple-primer/) to help with GPT-3 priming.

I'm excited to share some of the applications I build with this :)
